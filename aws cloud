                                                                                              AWS

 Accessing AWS: You can interact with AWS through:
       1. AWS Management Console (web-based GUI)
       2. AWS CLI (command-line interface)
       3. AWS SDKs (programmatically through supported programming languages)

On Premise
  On-premise refers to IT infrastructure, applications, and services that are hosted and managed within an organization’s own physical data centers, rather than on a public or private cloud.    All hardware, software, networking, security, and maintenance responsibilities lie with the organization itself. This setup gives companies complete control over their infrastructure and      data, which is often required for compliance or security reasons, but it also involves higher costs for setup, scaling, and ongoing management compared to cloud environments.

Datacenter
  Cloud computing is a model where computing resources like servers, storage, and software are delivered over the internet on a pay-as-you-go basis. Instead of relying on on-premise data        centers, organizations use cloud platforms for scalable and flexible infrastructure.

Cloud Service Models
There are three main cloud service models:
  1. IaaS (Infrastructure as a Service): Provides virtual servers, storage, and networking. You manage OS and apps; the provider manages hardware. Example: AWS EC2, S3, EBS, VPC.
  2. PaaS (Platform as a Service): Lets you build and run applications without managing the underlying infrastructure. Example: AWS Elastic Beanstalk, RDS, Lambda, Ddb. 
  3. SaaS (Software as a Service): Fully ready-to-use software delivered over the internet. Everything is managed by the provider. Example: Gmail, Salesforce, Zoom, Netflix.

Deployment Models
There are four main cloud deployment models:
  1. Public Cloud: Resources are owned and managed by a third-party provider and shared with multiple organizations. Example: AWS, Azure.
  2. Private Cloud: Infrastructure is dedicated to a single organization, either on-premise or hosted by a provider. Example: VMware vSphere private setup.
  3. Hybrid Cloud: Combines public and private clouds, allowing data and applications to move between them for flexibility and scalability.
  4. Community Cloud: Shared infrastructure for a specific community of organizations with common requirements, like security or compliance. Example: government or healthcare clouds.

What is Web Server and What is Application Server? Difference between them?
  1. A Web Server handles HTTP requests from clients, serving static content like HTML, CSS, and images. Examples are Apache HTTP Server and Nginx.
  2. An Application Server runs business logic and dynamic content, processing requests from clients and interacting with databases or other services. Examples are Tomcat, JBoss, and WebLogic.

  Key Differences:        Web Server                                    Application Server
    Purpose:    Web server → serves static content;             Application server → executes application logic.
    Processing: Web server handles HTTP requests;               Application server handles complex operations and backend logic.
    Examples:   Web server → Nginx, Apache;                     Application server → Tomcat, JBoss.

AWS Global Infrastructure
  1. Region: A Region is a physical location, like Mumbai, Singapore, or N. Virginia. Each Region is isolated from others, usually containing 2 or more Availability Zones. You choose a                    Region based on latency, compliance, and cost. Most AWS resources are Region-specific, though some services like IAM and Route 53 are global.
  2. Availability Zone (AZ): An AZ is one or more isolated data centers within a Region, designed for redundancy. AZs are close enough for fast replication but separated to avoid single                                    points of failure. Using multiple AZs allows highly available system design.
  3. Edge Location: These are small AWS sites near users to deliver content faster and handle DNS queries. Services like CloudFront, Route 53, and Lambda@Edge use Edge Locations to reduce                         latency, but they don’t host full applications.
  4. Local Zone (optional add-on): Extends a Region closer to end users to provide ultra-low latency for compute, storage, and other services.

Why we need multiple regions?
    Multiple regions are used to make applications resilient, compliant, and high-performing.
    1. Geographic distribution & latency: Placing resources closer to users reduces latency and improves performance.
    2. Compliance & data sovereignty: Certain regulations require data to stay within specific countries.
    3. Disaster recovery & business continuity: If one region fails due to a disaster or outage, workloads can run from another region.
    4. Fault isolation: Regions are fully independent, so failures in one won’t affect others.
    5. Load distribution: Workloads can be spread across regions to prevent overloading a single region.”


Why we need multiple AZs?
    Multiple Availability Zones (AZs) are used to ensure high availability, fault tolerance, and zero downtime within a region.    
    1. High Availability & Fault Tolerance: AZs are physically separate data centers with independent power, cooling, and networking, so if one fails, others continue running.
    2. Automatic Failover: Services like RDS Multi-AZ automatically maintain standby replicas and fail over seamlessly.
    3. Zero-Downtime Deployments & Maintenance: You can perform rolling updates or maintenance in one AZ without affecting application availability.
    4. Compliance: Multi-AZ setups help meet fault-tolerance requirements for certain industries.
    5. Local Zones: AWS also provides Local Zones in cities like Delhi to reduce latency for end users. These aren’t full regions but help deliver ultra-low-latency services closer to users,                      with plans to expand to other Indian metros.

Differences between AWS Local Zones and Edge Locations?
  AWS Local Zones and Edge Locations both bring AWS closer to users, but they serve different purposes:
  Purpose & Use Cases:
        Local Zones: Extend a region to provide compute and storage services closer to large population centers for ultra-low-latency applications, like gaming, media streaming, or real-time                       workloads.
        Edge Locations: Focus on content delivery and caching via CloudFront, DNS resolution with Route 53, and lightweight edge computing.
        
  Services Available:
        Local Zones: Provide core AWS services like EC2, EBS, VPC, ELB, and RDS—a subset of full region services.
        Edge Locations: Limited to CDN, Route 53, WAF, Shield, Lambda@Edge, and CloudFront Functions.

AWS Service: 
          These are collections of tools and functionalities that help users perform specific cloud tasks, like computing, storage, or networking. Examples include EC2 for compute, 
          S3 for storage, and RDS for databases.

AWS Resources :
          are the actual components you use to build and run applications in the cloud, such as EC2 instances, S3 buckets, or RDS databases. Services provide the functionality,
          while resources are the items you create and manage to use those services

What is EC2?When/Why to Use EC2 - Use Cases:
  Amazon EC2 (Elastic Compute Cloud) is a web service that provides resizable virtual servers, called instances, in the cloud. It allows you to run applications without managing physical       hardware.
  When/Why to Use EC2 – Use Cases:
        Web Hosting: Run websites or web applications.
        Application Hosting: Deploy custom business applications.
        Batch Processing & Compute-Intensive Workloads: Handle data processing, simulations, or machine learning tasks.
        Development & Testing Environments: Quickly provision and scale servers for testing or development.
        Disaster Recovery: Launch instances in another region or AZ for backup and failover.

Main EC2 Instance Families?
      Amazon EC2 instances are grouped into families based on workload needs:
        General Purpose: Balanced CPU, memory, and networking. Suitable for web servers and small databases. Examples: t3, t4g, m5.
        Compute Optimized: High CPU performance for compute-intensive tasks like batch processing or gaming servers. Examples: c5, c6g.
        Memory Optimized: High RAM for memory-intensive applications like databases, caching, and analytics. Examples: r5, x1e.
        Storage Optimized: High, fast storage for large-scale data processing, NoSQL databases, and data warehousing. Examples: i3, d2.
        Accelerated Computing: Includes GPUs or FPGAs for machine learning, graphics, or scientific computing. Examples: p4, g5.


How to Choose Instance Types?
  When selecting an EC2 instance, first consider the workload:
    Balanced workloads: General Purpose instances (t, m)
    CPU-heavy tasks: Compute Optimized (c)
    Memory-intensive applications: Memory Optimized (r, x)
    Storage-heavy workloads: Storage Optimized (i, d)
    AI/ML or graphics tasks: GPU/Accelerated instances (p, g)
  Next, consider scale and pricing: Start small for dev/test (e.g., t2.micro, t3.micro) and scale to enterprise workloads (e.g., m5, r5, c5) as needed, balancing performance with cost.


EC2 offers four main pricing models, balancing flexibility, commitment, and cost:
      1. On-Demand Instances: Pay per hour or second with no long-term commitment. Ideal for testing, dev, or unpredictable workloads. Example: t3.micro ≈ ₹0.85/hr. Like booking a taxi—you                                 pay only for what you use.
      2. Reserved Instances (RI): Commit for 1 or 3 years for specific instance types and regions. Up to 72% cheaper.
                  1. Standard RIs: Can change size within the same family or AZ.
                  2. Convertible RIs: Can switch to another instance of equal or higher value while keeping the term.
                        Use case: steady, predictable workloads like databases.
      3. Spot Instances: Use unused EC2 capacity at up to 90% discount. AWS can reclaim with 2-minute notice. Ideal for flexible, stateless, fault-tolerant workloads like big data or CI/CD.                          Example: c5.large on-demand ≈ $8/hr, spot ≈ $2/hr.
      4. Savings Plans: Commit to a $/hr spend for 1 or 3 years. Flexible across instance families, sizes, and regions, saving up to 66%. Any usage above the commitment is billed On-Demand.                            Like a monthly gym membership—you pay fixed, use any machine.
      These models let you optimize cost based on workload predictability and flexibility needs.”

      Example Scenario – Startup Website:
                Week 1: Dev/test → On-Demand
                After 3 months: Stable site → 1-yr Reserved for cost savings
                Batch video processing → Spot Instances
                Unsure about instance type → Savings Plan for discounted flexibility


What is SSL and What is TLS?
    SSL (Secure Sockets Layer) and TLS (Transport Layer Security) are cryptographic protocols used to secure communication over networks, like the internet. They encrypt data between clients     and servers to ensure confidentiality, integrity, and authentication.
        SSL: Older protocol, now largely deprecated due to security vulnerabilities.
        TLS: Successor to SSL, more secure and efficient. Modern websites and applications use TLS to encrypt data, authenticate servers, and optionally authenticate clients.

                                                                                            8th OCT

AMI
    Amazon machine image, its a prototype of a existing instance with all the configuration and s/w and application in it. ## You can say as a custom docker image
    Types of AMI
      1. AWS provided AMIs
      2. Custom AMIs
      3. Marketplace AMIs

Why are AMIs Important?
    Consistency → All servers look and behave the same.
    Speed → No need to manually install software every time.
    Scalability → Essential for Auto Scaling Groups.
    Disaster Recovery → Quickly re-launch failed instances.


    Practical,
              1. Select a EC2 (that u want to create a copy of) ----> Actions --> image and template --> Create image
                 image name --> "Check the box" Reboot instance( if u need data of "RAM" to be included in AMI) ----> *if required add volume by selecting snapshot of that volume -->                                          Create image

              2. Create Lunch template ---> Name of LT ----> "check box" if u want auto scale EC2 ---> select the AMI --->
                                            Select instance type (It will be for Auto scalled instance type) ----> Key pair name 
                                            --> Network settings ---> Storage ---> Tags ----> " Create lunch template"

   NOTE: 
        Lunch Template vs Lunch Configuration
          LT ---> its a modern mechnisum of lunching a template, where it provides a new feature called "Versioning" of Templates.
          LC ---> its a traditional way of lunching a template , where any changes/edit in image needed to made, it has to be from selecting a new instance itself from the begining

Elastic Load Balancing (ELB)

  What It Does
      Stands in front of your servers.
      Receives every user request.
      Distributes (balances) them across healthy EC2 instances.
      Checks if an instance is healthy; if not, stops sending traffic there.
      
  Key ELB Features (Conceptually)
      Traffic Distribution: spreads users to multiple servers.
      Health Checks: pings each instance (e.g., HTTP GET /) — removes unhealthy ones automatically.
      Automatic Scaling Ready: works with Auto Scaling Groups.
      High Availability: lives in multiple Availability Zones automatically.

ELB
  1. Traffic Distribution
    1. Round Robin (default for HTTP/HTTPS): each request goes to the next instance.
    2. Least Outstanding Requests (ALB): sends new traffic to the instance with least pending requests.
    3. Hash/IP based (NLB/ALB advanced): can stick a user to the same server if needed.
  
  2. Health Checks
      What happens:      
        The ELB constantly pings each target (EC2/IP/Lambda) on a specific protocol, port and path.
             Example: HTTP GET http://<server>/health.
        If the target fails (e.g., no response, 5xx errors), it’s marked unhealthy.
        ELB stops sending users to unhealthy targets until they recover.
      
  3. Automatic Scaling Ready
     What happens:
        The Target Group connected to the ELB can be backed by an Auto Scaling Group (ASG).
        When traffic grows and ASG adds new EC2 instances, they are automatically registered to the Target Group and start receiving traffic.
        When traffic drops, ASG terminates instances, and ELB stops sending requests to them.
  
  4. High Availability (Multi-AZ)
    What happens:
        ELB is designed to work across multiple Availability Zones (AZs) in a region.
        If one AZ (data center) goes down, ELB can still route traffic to servers in another AZ.
        Users keep getting service without knowing there was a failure.
  
  
Types of ELBs?
    1. Application Load Balancer (ALB)
          Use for: Websites & APIs (HTTP / HTTPS).
          Layer 7 aware: Understands web traffic.
          Smart routing: Can send requests to different servers based on URL/path/host.
          Most common for web apps.

    2. Network Load Balancer (NLB)
         Use for: Very fast, low-latency connections (TCP / UDP).
         Layer 4: Does not look inside HTTP, just forwards packets.
         Static IP support: Can use fixed public IPs.
         Great for non-web apps, gaming servers, databases, or when you need extreme performance.

   3. Gateway Load Balancer (GWLB)
         Gateway LB for advanced security appliances
         Newest type.
         Purpose: Put security appliances (firewalls, intrusion detection systems) behind a single entry point.
         Works at layer 3/4, uses GENEVE protocol to send traffic to virtual appliances.
         Not used for normal web apps — for network inspection / security setups.
      
   4. Classic LB- Legacy

  Auto Scaling Group:
  It automatically adjusts the number of EC2 instances based on traffic or load.
      If demand increases → it launches more instances.
      If demand drops → it terminates extra instances to save cost.

  Before scaling, you define the server setup using:
      AMI (Amazon Machine Image): Defines the OS and software.
      Launch Template / Configuration: Defines instance type, key pair, security groups, and storage.

  Auto Scaling Group (ASG) settings:
      Minimum capacity: Lowest number of instances to keep running (e.g., 2).
      Desired capacity: Target number of instances to maintain.
      Maximum capacity: Upper limit (e.g., 10) so it doesn’t scale endlessly.


 ASG Feature
   1.Scaling Policies – When to Add or Remove Servers
        a) Scale Out (Add Instances)
              Happens when load increases.
              Trigger: A metric crosses a threshold.
              Examples:
                  CPU > 70% for 5 minutes → add 1 instance.
                  Requests per target > 1000/sec → add 2 instances.
        b) Scale In (Remove Instances)
              Happens when load decreases.
              Trigger: A metric drops below a threshold.
              Example:
                  CPU < 30% for 10 minutes → remove 1 instance.
        
   2. Metrics used:
        1. CPU_Utilization
        2. RequestCountPerTarget
        3. Custom CloudWatch metrics

   3. Health Checks & Auto-Replacement
        The Auto Scaling Group (ASG) continuously monitors the health of instances:
            1.  EC2 Status Checks: Detects stopped, failed, or impaired instances.
            2.  ELB (Load Balancer) Health Checks: Periodically pings each instance (e.g., HTTP:80 /health).
                           If an instance doesn’t respond, the LB stops routing traffic to it.
If an instance is unhealthy, ASG terminates it and launches a new one automatically using the Launch Template.


What is TCP and UDP Protocol?
  TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) are two core transport layer protocols used to send data over the internet, but they work very differently.
      
  TCP is connection-oriented.
      It establishes a reliable connection between the sender and receiver before transmitting data. It guarantees that packets are delivered in the correct order and without errors. If any                        packet is lost, TCP retransmits it. Because of this reliability, TCP is slightly slower and has more overhead.
          Use cases: Web browsing (HTTP/HTTPS), file transfers (FTP), emails (SMTP), databases — basically, anything that needs guaranteed delivery.
      
      UDP is connectionless.
      It sends packets (called datagrams) without establishing a dedicated connection and doesn’t guarantee delivery, order, or error checking. It’s faster and has lower latency because                               there’s no handshake or retransmission mechanism. However, if packets are lost, the application itself must handle it.
            Use cases: Live video/audio streaming, gaming, DNS queries — situations where speed matters more than perfect accuracy.
      
      In short:
            TCP = Reliable, Ordered, Connection-based, Slower
            UDP = Fast, Lightweight, Unreliable (no guarantees), Best-effort



Security Group
    A Security Group in AWS acts as a virtual firewall for your EC2 instances, controlling both inbound and outbound traffic at the instance level. It’s attached to the Elastic Network              Interface (ENI) or directly to the instance, and AWS evaluates it for every packet entering or leaving.
    
    One of the most important characteristics of Security Groups is that they’re stateful. This means if an inbound request is allowed, the corresponding outbound response is automatically          allowed, even if there isn’t a specific outbound rule for it. The same applies in reverse.

    Security Groups consist of inbound and outbound rules:
           1. Inbound rules define what kind of traffic can enter the instance — for example, allowing HTTP traffic from anywhere or SSH only from a specific admin IP.
           2. Outbound rules define what traffic the instance can send out.
    Each rule specifies the protocol, port range, traffic type, and the source or destination, which can be an IP range (CIDR) or another security group.
    
    A key point to remember is that Security Groups are allow-only. You can only add “allow” rules; there are no explicit “deny” rules. If something isn’t allowed, it’s automatically denied.        Also, any changes made to a Security Group are applied in real time to all attached instances.
    
    For example, a typical SSH rule might look like this:
              Type: SSH
              Protocol: TCP
              Port range: 22
              Source: 203.0.113.25/32 (specific admin IP)
              Description: Admin SSH Access


 ENI
    An Elastic Network Interface (ENI) is a virtual network interface in AWS that provides network connectivity for EC2 instances. Think of it as a virtual network card. Every EC2 instance         has at least one primary ENI attached by default, and you can attach additional ENIs based on your architecture requirements.

    An ENI contains several key elements:
        1. A primary private IPv4 address (mandatory) and optionally secondary private IP addresses.
        2. A public IPv4 address if auto-assigned or an Elastic IP if you want a fixed public IP.
        3. One or more Security Groups that define inbound and outbound traffic rules.
        4. A unique MAC address, and optionally IPv6 addresses if enabled.
    
    There are two types of ENIs:
       1. Primary ENI: This is automatically created when you launch an EC2 instance. It can’t be detached because it’s the main network interface.
       2. Secondary ENIs: These can be created manually and attached or detached from instances within the same subnet and Availability Zone. They can even be moved between instances.
    
    Why use multiple ENIs?
    1. Traffic Segregation: For example, you can attach two ENIs to a single instance — one for management traffic and another for application traffic.
       1. The management ENI can be placed in a private subnet accessible only through VPN or internal networks, restricted to SSH or RDP for administrators.    
       2. The application ENI can be placed in a public subnet with a public or Elastic IP, serving web traffic on HTTP/HTTPS ports. This separation improves security and network control.
    
    2. High Availability & Failover: If an instance fails, you can detach the ENI and attach it to a replacement instance. Because the ENI retains its private IP, public IP (if Elastic), and            MAC address, this provides a seamless recovery without forcing clients to update IPs.
    3. Multi-homing: You can assign multiple IPs to a single instance, which is useful for scenarios like hosting multiple websites on one server.


EIP
  An Elastic IP (EIP) in AWS is a static, public IPv4 address that you can allocate to your AWS account and associate with EC2 instances or network interfaces. Unlike a regular public IP,        which changes if you stop or terminate the instance, an Elastic IP remains fixed until you explicitly release it. This makes it ideal for scenarios where you need a consistent, publicly      reachable IP address, such as hosting production websites, configuring DNS records, or enabling failover setups.

  The main reason AWS differentiates between normal public IPs and EIPs is because public IPv4 addresses are limited resources. By default, AWS assigns dynamic public IPs so that they can be     recycled when not in use, helping optimize IP address utilization and keeping costs low for users who don’t require static addresses.

  However, Elastic IPs are chargeable if allocated but not in use. For example, if you reserve an EIP but don’t associate it with a running resource, AWS charges you to discourage waste of       IPs.

  Typical use cases for EIPs include:
    Ensuring a fixed IP for production servers, so DNS records don’t need frequent updates.
    Enabling quick recovery and failover by detaching the EIP from a failed instance and reattaching it to a standby instance, without changing client configurations.




Why Do We Need Elastic IPs (EIPs)?
   Elastic IPs provide a stable, public-facing IP address for your AWS resources. Unlike standard public IPs that can change when an instance is stopped or restarted, EIPs remain constant        until you release them. This stability is essential for production workloads and external connectivity.

   Key Reasons to Use Elastic IPs:
    
    Firewall Whitelisting:
    Many clients or external systems only accept traffic from pre-approved IP addresses. Using an EIP ensures your instance’s IP remains consistent, avoiding disruptions caused by changing         public IPs.
    
    DNS Stability:
    DNS records need to point to a fixed IP. Without an EIP, you would have to update DNS records every time an instance restarts. Elastic IPs eliminate this issue, keeping your DNS mappings       stable.
    
    Failover & High Availability:
    If an instance fails, you can disassociate the EIP from the failed instance and attach it to a standby or replacement instance in the same subnet/AZ. The new instance immediately               receives the same public IP, ensuring uninterrupted connectivity without updating DNS or firewall rules.
    
    Internet Access for Private IPs:
    To allow external access, each private IP on an ENI must be mapped to a public IP. Elastic IPs provide this mapping reliably.
  
  Rules and Limits:
     1. Each Elastic IP can be associated with one private IP per ENI.
     2. AWS provides 5 Elastic IPs per region by default (you can request more if needed).
     3. Unused EIPs incur charges, so release them back to AWS when not in use to avoid unnecessary costs.



What is HTTP, HTTPS, SSH, RDP, FTP, SMTP?
    1. HTTP (Hypertext Transfer Protocol):
      Protocol used for transferring web pages over the internet.
      Port: 80
      Unsecured: Data is sent in plain text, so it can be intercepted.
      Use case: Standard website browsing where encryption isn’t critical.
      
    2. HTTPS (Hypertext Transfer Protocol Secure):
      Secure version of HTTP using TLS/SSL encryption.
      Port: 443
      Encrypted & authenticated: Ensures data confidentiality, integrity, and server authentication.
      Use case: Online banking, e-commerce, and any site handling sensitive data.
      
    3. SSH (Secure Shell):
      Protocol for secure remote access to servers over a network.
      Port: 22
      Encrypted: Commands and data are encrypted end-to-end.
      Use case: Admin access to Linux/Unix servers for management and configuration.
      
    4. RDP (Remote Desktop Protocol):
      Microsoft protocol for remote desktop access to Windows machines.
      Port: 3389
      Encrypted: Can use network-level authentication for security.
      Use case: Remote administration or user access to Windows desktops/servers.
      
    5. FTP (File Transfer Protocol):
      Protocol for transferring files between client and server.
      Ports: 20 (data), 21 (control)
      Unsecured by default: Credentials and data are in plain text. Secure alternatives include SFTP or FTPS.
      Use case: Uploading/downloading website files or backups.
      
    6. SMTP (Simple Mail Transfer Protocol):
      Protocol for sending emails between servers.
      Port: 25 (standard), 587 (submission), 465 (SMTPS)
      Use case: Email delivery from applications or mail servers to other mail servers.




Common Protocols and Ports:
      
      HTTP:           80 (used for unsecured web traffic)
      HTTPS:          443 (secure web traffic via TLS/SSL)
      SSH:            22 (secure remote login for Linux/Unix servers)
      RDP:            3389 (remote desktop access for Windows)
      FTP:            21 (control), 20 (data) – unencrypted by default; SFTP/FTPS are secure alternatives
      SMTP:           25 (server-to-server mail transfer), 587 (mail submission), 465 (secure SMTP/SMTPS)
      DNS:            53 (UDP for queries, TCP for zone transfers)
      Telnet:         23 (unencrypted remote access – largely obsolete)
      IMAP:           143 (email retrieval), 993 (IMAP over SSL)
      POP3:           110 (email retrieval), 995 (POP3 over SSL)
      MySQL:          3306 (database access)
      PostgreSQL:     5432 (database access)


KEY-PAIR

Key Pairs & SSH Access in AWS

What is a Key Pair?
    A Key Pair is a set of cryptographic security credentials used to securely connect to your EC2 instances. It functions like a lock-and-key system:
    
    Private Key (.pem or .ppk file):
      Stored securely on your local machine.
      Never shared or uploaded to AWS.
        
    Public Key:
      Stored in AWS and automatically injected into the EC2 instance during launch.
      Works with the private key to authenticate your connection.


Why Key Pairs Are Important:    
  Security:
    Only someone with the private key can access the instance. This is much safer than using a username/password combination.
    
  Authentication:
    Verifies that the person connecting is the rightful owner of the key pair.
    
  One-Time Setup:
    AWS provides the private key only once when creating the pair.
      Important: If you lose it, you cannot download it again—you must create a new key pair and update the instance configuration.


Ways to Connect to a Linux EC2 Instance

1) SSH (Secure Shell) — Most Common Method
    
    Description: Traditional method using port 22 and a private key (.pem).
    Requirements:
        EC2 must have a public IP or Elastic IP.
        Port 22 open in the Security Group.
        Private key (.pem) downloaded from AWS.
    Command:
        ssh -i my-key.pem ec2-user@<Public-IP>
    Use case: Standard admin access for most Linux instances.
    
2) EC2 Instance Connect (Browser-based SSH)
    
    Description: Connect directly from the AWS Console using temporary keys managed by AWS.
    Requirements: IAM permissions for EC2 Instance Connect, instance allows SSH (port 22).
    Pros: No PEM file required, works from anywhere, temporary credentials for security.
    Cons: IAM setup needed; still requires SSH port open.
    How to use: EC2 Console → Select instance → Connect → EC2 Instance Connect → Click “Connect”.
    
3) Session Manager (AWS Systems Manager)
    
    Description: Secure connection without SSH, public IP, or open ports. Works via SSM Agent.
    Requirements:
        SSM Agent installed (default on Amazon Linux 2 / Ubuntu).
        IAM role with AmazonSSMManagedInstanceCore policy.
        Outbound internet access (via NAT Gateway or VPC Endpoint).
    Pros: No key pairs, no open SSH ports, IAM-controlled, logs auditable via CloudTrail/CloudWatch.
    Use case: Enterprise/production environments, private subnets.
    Command:
        aws ssm start-session --target <instance-id>
    
4) AWS CloudShell (Console Terminal)
    
    Description: Browser-based terminal in AWS Console.
    Use: Execute CLI commands to connect to EC2 via Instance Connect or Session Manager.
    Example:
    aws ssm start-session --target <instance-id>  
        
5) EC2 Serial Console (Advanced / Troubleshooting)
    Description: Direct console access similar to a physical serial port.
    Requirements:
          Enable EC2 Serial Console in account settings.
          Works only on Nitro-based instances.
          IAM permission: ec2-instance-connect:SendSerialConsoleSSHPublicKey.
    Use case: Troubleshoot boot issues, firewall misconfigurations, or failed SSH/SSM connections.


